\chapter{Background knowledge: Recurrent networks and LSTM}
	\label{CH_02}

\section{Recurrent networks}
Different from other networks such as feed forward neural network(FFNN) and convolution neural network, recurrent network works well on problems that have sequential input such as speech recognition, language modeling, translation and image captioning. Figure \ref{fig:0} shows how a typical recurrent network looks like. A loop in the network allows it to pass the information from previous steps to the network to make a decision for the current input. In contrast to the fix length context window used in FFNN, RNN stores the activation of previous time step in hidden state and it provide context information within a undefined window size.

\begin{figure}[h] 
	\centering
	\includegraphics[width=2.0in]{Figures/recurrent1}
	\caption[recurrent network]{Illustration of recurrent network}
	\label{fig:0}
\end{figure}

However, training conventional RNNs using back-propagation technique is difficult due to the vanishing gradient and exploding gradient problems\cite{bengio1994learning}. The gradient shifting problem make
also make it difficult for the network to remember long time dependency that longer than 5-10 time steps between input and output.
\section{Long Short term memory}
In order to address the problem above, a special recurrent network architecture Long-Short-Term Memory(LSTM) have been proposed\cite{hochreiter1997long}. LSTM and it's variations have been successfully applied to sequence prediction, translation and sequence labeling tasks. In the rest of this chapter I will introduce how LSTM and one of its variantion GRU(gated recurrent unit) works.\par

\subsection{Long short term memory}
To make it easier to understand the idea of LSTM, let unroll the loop in the network and then we got a fix length static version of the same network(Figure \ref{fig:1}). The operation in the two RNN version are the same, the difference is the static version only take fix length input and don't have to consider when to stop the iteration thus it runs faster.\par
\begin{figure}[h] 
	\centering
	\includegraphics[width=6.0in]{Figures/recurrent2}
	\caption[unrolled recurrent network]{Illustration of recurrent network}
	\label{fig:1}
\end{figure}

When we look at the inside of each recurrent unit(Cell), a conventional RNN can be represent as Figure \ref{fig:2}. The cell take the concatenation of input of time t and output of time t-1 as input. Going through a dense layer the cell will output the activation of the dense layer.\par  

LSTM also have the same loop structure of conventional RNN, but it has different structure inside the cell. Instead of having only a single dense layer in the cell, the LSTM cell has four dense layers. One of them is the counter-part in conventional RNN cell, the rest three, however, works as three "gates" controlling the behavior of the cell(Figure \ref{fig:3}).\par 

\begin{figure}[h] 
	\centering
	\includegraphics[width=6.0in]{Figures/recurrent3}
	\caption[Detail inside recurrent unit]{Illustration of recurrent network}
	\label{fig:2}
\end{figure}

\begin{figure}[h] 
	\centering
	\includegraphics[width=6.0in]{Figures/recurrent4}
	\caption[Detail inside LSTM]{Illustration of recurrent network}
	\label{fig:3}
\end{figure}

The first step of calculating the output $h_t$ of each time step is updating the cell state $C_t$. Because the output of each time step, is essentially the cell state masked by a weight matrix. In order to update the cell state, we need the output of the first three dense layer inside the cell which are forget gate layer, input gate layer and $tanh$ layer. The activation of these three layers can be calculated by using \ref{eq:0}, \ref{eq:1} and \ref{eq:2}. Then according to  \ref{eq:3} the network will decide how to update the cell state by forgetting some of the old state and remember some of the new input state.\par
After updated the cell state, the network will calculate the output of this time step using \ref{eq:4} and \ref{eq:5}. More specifically speaking, \ref{eq:4} calculate the output gate weights $o_t$ and  \ref{eq:5} apply the weight to the activation of cell state. 
\begin{subequations} 
    \begin{align}
    	f_{ t }&=\sigma (W_{ f }\cdot [h_{t-1} , x_t] +b_f) \label{eq:0}\\	
        i_t &= \sigma(W_i \cdot[h_{t-1}, x_t] + b_i)\label{eq:1}\\
        \tilde {  {C _t } } &= tanh(W_c\cdot [h_{t-1}, x_t]+ b_c) \label{eq:2}
    \end{align}	
\end{subequations}




\begin{equation} \label{eq:3}
    C_t = f_t\ast C_{t-1} +i_t\ast \tilde{C}
\end{equation}

\begin{subequations}
    \begin{align}
        o_t &= \sigma(W_o\cdot [h_{t-1}, x_t] +b_o) \label{eq:4}\\
        h_t &= o_t \ast tanh(C_t) \label{eq:5}
    \end{align}
\end{subequations}

\par


\subsection{Gated recurrent unit}
\par
The gate recurrent unit(GRU)\cite{cho2014learning} is a variance of LSTM. In fact there are many different version of LSTM. The version described in previous chapter is the simplest one. One problem of this LSTM version is that the number of parameters is about 4 times of a conventional recurrent network, because it have four dense layers inside the cell. And it have to keep the cell state which will consume more memory. These problems slow down the speed of the network. The GRU is a special variance of LSTM that only has three dense layer and doesn't have the cell state.\par

\begin{figure}[h] 
	\centering
	\includegraphics[width=6.0in]{Figures/recurrent5}
	\caption[unrolled recurrent network]{Illustration of recurrent network}
	\label{fig:4}
\end{figure}

As we can see in Figure \ref{fig:4}, GRU merge the input gate and forget gate in LSTM cell into a single update gate $z_t$. The update gate $z_t$ selects whether the hidden state is to be updated with
a new hidden state $\tilde{h}$. The reset gate $r_t$ decides whether the previous hidden state is ignored. See Eqs. \ref{eq:GRU0}-\ref{eq:GRU1} for the detailed equations of $r, z, h$ and $\tilde{h}$.

\begin{subequations} 
    \begin{align}
    	z_t &= \sigma(W_z\cdot [h_{t-1}, x_t]) \label{eq:GRU0}\\ 
        r_t &= \sigma(W_r\cdot [h_{t-1}, x_t]) \label{eq:GRU1}\\
        \tilde{h} &= tanh(W\cdot [r_t\ast h_{t-1}, x_t]) \label{eq:GRU2}\\
        h_t &= (1 -z_t)\ast h_{t-1} + z_t\ast \tilde{h} \label{eq:GRU3}
    \end{align}	
\end{subequations}